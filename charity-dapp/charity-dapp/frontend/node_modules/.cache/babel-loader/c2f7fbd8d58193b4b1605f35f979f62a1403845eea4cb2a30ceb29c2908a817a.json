{"ast":null,"code":"/**\n * @module LRUCache\n */\nconst perf = typeof performance === 'object' && performance && typeof performance.now === 'function' ? performance : Date;\nconst warned = new Set();\n/* c8 ignore start */\nconst PROCESS = typeof process === 'object' && !!process ? process : {};\n/* c8 ignore start */\nconst emitWarning = (msg, type, code, fn) => {\n  typeof PROCESS.emitWarning === 'function' ? PROCESS.emitWarning(msg, type, code, fn) : console.error(`[${code}] ${type}: ${msg}`);\n};\nlet AC = globalThis.AbortController;\nlet AS = globalThis.AbortSignal;\n/* c8 ignore start */\nif (typeof AC === 'undefined') {\n  //@ts-ignore\n  AS = class AbortSignal {\n    onabort;\n    _onabort = [];\n    reason;\n    aborted = false;\n    addEventListener(_, fn) {\n      this._onabort.push(fn);\n    }\n  };\n  //@ts-ignore\n  AC = class AbortController {\n    constructor() {\n      warnACPolyfill();\n    }\n    signal = new AS();\n    abort(reason) {\n      if (this.signal.aborted) return;\n      //@ts-ignore\n      this.signal.reason = reason;\n      //@ts-ignore\n      this.signal.aborted = true;\n      //@ts-ignore\n      for (const fn of this.signal._onabort) {\n        fn(reason);\n      }\n      this.signal.onabort?.(reason);\n    }\n  };\n  let printACPolyfillWarning = PROCESS.env?.LRU_CACHE_IGNORE_AC_WARNING !== '1';\n  const warnACPolyfill = () => {\n    if (!printACPolyfillWarning) return;\n    printACPolyfillWarning = false;\n    emitWarning('AbortController is not defined. If using lru-cache in ' + 'node 14, load an AbortController polyfill from the ' + '`node-abort-controller` package. A minimal polyfill is ' + 'provided for use by LRUCache.fetch(), but it should not be ' + 'relied upon in other contexts (eg, passing it to other APIs that ' + 'use AbortController/AbortSignal might have undesirable effects). ' + 'You may disable this with LRU_CACHE_IGNORE_AC_WARNING=1 in the env.', 'NO_ABORT_CONTROLLER', 'ENOTSUP', warnACPolyfill);\n  };\n}\n/* c8 ignore stop */\nconst shouldWarn = code => !warned.has(code);\nconst TYPE = Symbol('type');\nconst isPosInt = n => n && n === Math.floor(n) && n > 0 && isFinite(n);\n/* c8 ignore start */\n// This is a little bit ridiculous, tbh.\n// The maximum array length is 2^32-1 or thereabouts on most JS impls.\n// And well before that point, you're caching the entire world, I mean,\n// that's ~32GB of just integers for the next/prev links, plus whatever\n// else to hold that many keys and values.  Just filling the memory with\n// zeroes at init time is brutal when you get that big.\n// But why not be complete?\n// Maybe in the future, these limits will have expanded.\nconst getUintArray = max => !isPosInt(max) ? null : max <= Math.pow(2, 8) ? Uint8Array : max <= Math.pow(2, 16) ? Uint16Array : max <= Math.pow(2, 32) ? Uint32Array : max <= Number.MAX_SAFE_INTEGER ? ZeroArray : null;\n/* c8 ignore stop */\nclass ZeroArray extends Array {\n  constructor(size) {\n    super(size);\n    this.fill(0);\n  }\n}\nclass Stack {\n  heap;\n  length;\n  // private constructor\n  static #constructing = false;\n  static create(max) {\n    const HeapCls = getUintArray(max);\n    if (!HeapCls) return [];\n    Stack.#constructing = true;\n    const s = new Stack(max, HeapCls);\n    Stack.#constructing = false;\n    return s;\n  }\n  constructor(max, HeapCls) {\n    /* c8 ignore start */\n    if (!Stack.#constructing) {\n      throw new TypeError('instantiate Stack using Stack.create(n)');\n    }\n    /* c8 ignore stop */\n    this.heap = new HeapCls(max);\n    this.length = 0;\n  }\n  push(n) {\n    this.heap[this.length++] = n;\n  }\n  pop() {\n    return this.heap[--this.length];\n  }\n}\n/**\n * Default export, the thing you're using this module to get.\n *\n * The `K` and `V` types define the key and value types, respectively. The\n * optional `FC` type defines the type of the `context` object passed to\n * `cache.fetch()` and `cache.memo()`.\n *\n * Keys and values **must not** be `null` or `undefined`.\n *\n * All properties from the options object (with the exception of `max`,\n * `maxSize`, `fetchMethod`, `memoMethod`, `dispose` and `disposeAfter`) are\n * added as normal public members. (The listed options are read-only getters.)\n *\n * Changing any of these will alter the defaults for subsequent method calls.\n */\nexport class LRUCache {\n  // options that cannot be changed without disaster\n  #max;\n  #maxSize;\n  #dispose;\n  #disposeAfter;\n  #fetchMethod;\n  #memoMethod;\n  /**\n   * {@link LRUCache.OptionsBase.ttl}\n   */\n  ttl;\n  /**\n   * {@link LRUCache.OptionsBase.ttlResolution}\n   */\n  ttlResolution;\n  /**\n   * {@link LRUCache.OptionsBase.ttlAutopurge}\n   */\n  ttlAutopurge;\n  /**\n   * {@link LRUCache.OptionsBase.updateAgeOnGet}\n   */\n  updateAgeOnGet;\n  /**\n   * {@link LRUCache.OptionsBase.updateAgeOnHas}\n   */\n  updateAgeOnHas;\n  /**\n   * {@link LRUCache.OptionsBase.allowStale}\n   */\n  allowStale;\n  /**\n   * {@link LRUCache.OptionsBase.noDisposeOnSet}\n   */\n  noDisposeOnSet;\n  /**\n   * {@link LRUCache.OptionsBase.noUpdateTTL}\n   */\n  noUpdateTTL;\n  /**\n   * {@link LRUCache.OptionsBase.maxEntrySize}\n   */\n  maxEntrySize;\n  /**\n   * {@link LRUCache.OptionsBase.sizeCalculation}\n   */\n  sizeCalculation;\n  /**\n   * {@link LRUCache.OptionsBase.noDeleteOnFetchRejection}\n   */\n  noDeleteOnFetchRejection;\n  /**\n   * {@link LRUCache.OptionsBase.noDeleteOnStaleGet}\n   */\n  noDeleteOnStaleGet;\n  /**\n   * {@link LRUCache.OptionsBase.allowStaleOnFetchAbort}\n   */\n  allowStaleOnFetchAbort;\n  /**\n   * {@link LRUCache.OptionsBase.allowStaleOnFetchRejection}\n   */\n  allowStaleOnFetchRejection;\n  /**\n   * {@link LRUCache.OptionsBase.ignoreFetchAbort}\n   */\n  ignoreFetchAbort;\n  // computed properties\n  #size;\n  #calculatedSize;\n  #keyMap;\n  #keyList;\n  #valList;\n  #next;\n  #prev;\n  #head;\n  #tail;\n  #free;\n  #disposed;\n  #sizes;\n  #starts;\n  #ttls;\n  #hasDispose;\n  #hasFetchMethod;\n  #hasDisposeAfter;\n  /**\n   * Do not call this method unless you need to inspect the\n   * inner workings of the cache.  If anything returned by this\n   * object is modified in any way, strange breakage may occur.\n   *\n   * These fields are private for a reason!\n   *\n   * @internal\n   */\n  static unsafeExposeInternals(c) {\n    return {\n      // properties\n      starts: c.#starts,\n      ttls: c.#ttls,\n      sizes: c.#sizes,\n      keyMap: c.#keyMap,\n      keyList: c.#keyList,\n      valList: c.#valList,\n      next: c.#next,\n      prev: c.#prev,\n      get head() {\n        return c.#head;\n      },\n      get tail() {\n        return c.#tail;\n      },\n      free: c.#free,\n      // methods\n      isBackgroundFetch: p => c.#isBackgroundFetch(p),\n      backgroundFetch: (k, index, options, context) => c.#backgroundFetch(k, index, options, context),\n      moveToTail: index => c.#moveToTail(index),\n      indexes: options => c.#indexes(options),\n      rindexes: options => c.#rindexes(options),\n      isStale: index => c.#isStale(index)\n    };\n  }\n  // Protected read-only members\n  /**\n   * {@link LRUCache.OptionsBase.max} (read-only)\n   */\n  get max() {\n    return this.#max;\n  }\n  /**\n   * {@link LRUCache.OptionsBase.maxSize} (read-only)\n   */\n  get maxSize() {\n    return this.#maxSize;\n  }\n  /**\n   * The total computed size of items in the cache (read-only)\n   */\n  get calculatedSize() {\n    return this.#calculatedSize;\n  }\n  /**\n   * The number of items stored in the cache (read-only)\n   */\n  get size() {\n    return this.#size;\n  }\n  /**\n   * {@link LRUCache.OptionsBase.fetchMethod} (read-only)\n   */\n  get fetchMethod() {\n    return this.#fetchMethod;\n  }\n  get memoMethod() {\n    return this.#memoMethod;\n  }\n  /**\n   * {@link LRUCache.OptionsBase.dispose} (read-only)\n   */\n  get dispose() {\n    return this.#dispose;\n  }\n  /**\n   * {@link LRUCache.OptionsBase.disposeAfter} (read-only)\n   */\n  get disposeAfter() {\n    return this.#disposeAfter;\n  }\n  constructor(options) {\n    const {\n      max = 0,\n      ttl,\n      ttlResolution = 1,\n      ttlAutopurge,\n      updateAgeOnGet,\n      updateAgeOnHas,\n      allowStale,\n      dispose,\n      disposeAfter,\n      noDisposeOnSet,\n      noUpdateTTL,\n      maxSize = 0,\n      maxEntrySize = 0,\n      sizeCalculation,\n      fetchMethod,\n      memoMethod,\n      noDeleteOnFetchRejection,\n      noDeleteOnStaleGet,\n      allowStaleOnFetchRejection,\n      allowStaleOnFetchAbort,\n      ignoreFetchAbort\n    } = options;\n    if (max !== 0 && !isPosInt(max)) {\n      throw new TypeError('max option must be a nonnegative integer');\n    }\n    const UintArray = max ? getUintArray(max) : Array;\n    if (!UintArray) {\n      throw new Error('invalid max value: ' + max);\n    }\n    this.#max = max;\n    this.#maxSize = maxSize;\n    this.maxEntrySize = maxEntrySize || this.#maxSize;\n    this.sizeCalculation = sizeCalculation;\n    if (this.sizeCalculation) {\n      if (!this.#maxSize && !this.maxEntrySize) {\n        throw new TypeError('cannot set sizeCalculation without setting maxSize or maxEntrySize');\n      }\n      if (typeof this.sizeCalculation !== 'function') {\n        throw new TypeError('sizeCalculation set to non-function');\n      }\n    }\n    if (memoMethod !== undefined && typeof memoMethod !== 'function') {\n      throw new TypeError('memoMethod must be a function if defined');\n    }\n    this.#memoMethod = memoMethod;\n    if (fetchMethod !== undefined && typeof fetchMethod !== 'function') {\n      throw new TypeError('fetchMethod must be a function if specified');\n    }\n    this.#fetchMethod = fetchMethod;\n    this.#hasFetchMethod = !!fetchMethod;\n    this.#keyMap = new Map();\n    this.#keyList = new Array(max).fill(undefined);\n    this.#valList = new Array(max).fill(undefined);\n    this.#next = new UintArray(max);\n    this.#prev = new UintArray(max);\n    this.#head = 0;\n    this.#tail = 0;\n    this.#free = Stack.create(max);\n    this.#size = 0;\n    this.#calculatedSize = 0;\n    if (typeof dispose === 'function') {\n      this.#dispose = dispose;\n    }\n    if (typeof disposeAfter === 'function') {\n      this.#disposeAfter = disposeAfter;\n      this.#disposed = [];\n    } else {\n      this.#disposeAfter = undefined;\n      this.#disposed = undefined;\n    }\n    this.#hasDispose = !!this.#dispose;\n    this.#hasDisposeAfter = !!this.#disposeAfter;\n    this.noDisposeOnSet = !!noDisposeOnSet;\n    this.noUpdateTTL = !!noUpdateTTL;\n    this.noDeleteOnFetchRejection = !!noDeleteOnFetchRejection;\n    this.allowStaleOnFetchRejection = !!allowStaleOnFetchRejection;\n    this.allowStaleOnFetchAbort = !!allowStaleOnFetchAbort;\n    this.ignoreFetchAbort = !!ignoreFetchAbort;\n    // NB: maxEntrySize is set to maxSize if it's set\n    if (this.maxEntrySize !== 0) {\n      if (this.#maxSize !== 0) {\n        if (!isPosInt(this.#maxSize)) {\n          throw new TypeError('maxSize must be a positive integer if specified');\n        }\n      }\n      if (!isPosInt(this.maxEntrySize)) {\n        throw new TypeError('maxEntrySize must be a positive integer if specified');\n      }\n      this.#initializeSizeTracking();\n    }\n    this.allowStale = !!allowStale;\n    this.noDeleteOnStaleGet = !!noDeleteOnStaleGet;\n    this.updateAgeOnGet = !!updateAgeOnGet;\n    this.updateAgeOnHas = !!updateAgeOnHas;\n    this.ttlResolution = isPosInt(ttlResolution) || ttlResolution === 0 ? ttlResolution : 1;\n    this.ttlAutopurge = !!ttlAutopurge;\n    this.ttl = ttl || 0;\n    if (this.ttl) {\n      if (!isPosInt(this.ttl)) {\n        throw new TypeError('ttl must be a positive integer if specified');\n      }\n      this.#initializeTTLTracking();\n    }\n    // do not allow completely unbounded caches\n    if (this.#max === 0 && this.ttl === 0 && this.#maxSize === 0) {\n      throw new TypeError('At least one of max, maxSize, or ttl is required');\n    }\n    if (!this.ttlAutopurge && !this.#max && !this.#maxSize) {\n      const code = 'LRU_CACHE_UNBOUNDED';\n      if (shouldWarn(code)) {\n        warned.add(code);\n        const msg = 'TTL caching without ttlAutopurge, max, or maxSize can ' + 'result in unbounded memory consumption.';\n        emitWarning(msg, 'UnboundedCacheWarning', code, LRUCache);\n      }\n    }\n  }\n  /**\n   * Return the number of ms left in the item's TTL. If item is not in cache,\n   * returns `0`. Returns `Infinity` if item is in cache without a defined TTL.\n   */\n  getRemainingTTL(key) {\n    return this.#keyMap.has(key) ? Infinity : 0;\n  }\n  #initializeTTLTracking() {\n    const ttls = new ZeroArray(this.#max);\n    const starts = new ZeroArray(this.#max);\n    this.#ttls = ttls;\n    this.#starts = starts;\n    this.#setItemTTL = (index, ttl, start = perf.now()) => {\n      starts[index] = ttl !== 0 ? start : 0;\n      ttls[index] = ttl;\n      if (ttl !== 0 && this.ttlAutopurge) {\n        const t = setTimeout(() => {\n          if (this.#isStale(index)) {\n            this.#delete(this.#keyList[index], 'expire');\n          }\n        }, ttl + 1);\n        // unref() not supported on all platforms\n        /* c8 ignore start */\n        if (t.unref) {\n          t.unref();\n        }\n        /* c8 ignore stop */\n      }\n    };\n    this.#updateItemAge = index => {\n      starts[index] = ttls[index] !== 0 ? perf.now() : 0;\n    };\n    this.#statusTTL = (status, index) => {\n      if (ttls[index]) {\n        const ttl = ttls[index];\n        const start = starts[index];\n        /* c8 ignore next */\n        if (!ttl || !start) return;\n        status.ttl = ttl;\n        status.start = start;\n        status.now = cachedNow || getNow();\n        const age = status.now - start;\n        status.remainingTTL = ttl - age;\n      }\n    };\n    // debounce calls to perf.now() to 1s so we're not hitting\n    // that costly call repeatedly.\n    let cachedNow = 0;\n    const getNow = () => {\n      const n = perf.now();\n      if (this.ttlResolution > 0) {\n        cachedNow = n;\n        const t = setTimeout(() => cachedNow = 0, this.ttlResolution);\n        // not available on all platforms\n        /* c8 ignore start */\n        if (t.unref) {\n          t.unref();\n        }\n        /* c8 ignore stop */\n      }\n      return n;\n    };\n    this.getRemainingTTL = key => {\n      const index = this.#keyMap.get(key);\n      if (index === undefined) {\n        return 0;\n      }\n      const ttl = ttls[index];\n      const start = starts[index];\n      if (!ttl || !start) {\n        return Infinity;\n      }\n      const age = (cachedNow || getNow()) - start;\n      return ttl - age;\n    };\n    this.#isStale = index => {\n      const s = starts[index];\n      const t = ttls[index];\n      return !!t && !!s && (cachedNow || getNow()) - s > t;\n    };\n  }\n  // conditionally set private methods related to TTL\n  #updateItemAge = () => {};\n  #statusTTL = () => {};\n  #setItemTTL = () => {};\n  /* c8 ignore stop */\n  #isStale = () => false;\n  #initializeSizeTracking() {\n    const sizes = new ZeroArray(this.#max);\n    this.#calculatedSize = 0;\n    this.#sizes = sizes;\n    this.#removeItemSize = index => {\n      this.#calculatedSize -= sizes[index];\n      sizes[index] = 0;\n    };\n    this.#requireSize = (k, v, size, sizeCalculation) => {\n      // provisionally accept background fetches.\n      // actual value size will be checked when they return.\n      if (this.#isBackgroundFetch(v)) {\n        return 0;\n      }\n      if (!isPosInt(size)) {\n        if (sizeCalculation) {\n          if (typeof sizeCalculation !== 'function') {\n            throw new TypeError('sizeCalculation must be a function');\n          }\n          size = sizeCalculation(v, k);\n          if (!isPosInt(size)) {\n            throw new TypeError('sizeCalculation return invalid (expect positive integer)');\n          }\n        } else {\n          throw new TypeError('invalid size value (must be positive integer). ' + 'When maxSize or maxEntrySize is used, sizeCalculation ' + 'or size must be set.');\n        }\n      }\n      return size;\n    };\n    this.#addItemSize = (index, size, status) => {\n      sizes[index] = size;\n      if (this.#maxSize) {\n        const maxSize = this.#maxSize - sizes[index];\n        while (this.#calculatedSize > maxSize) {\n          this.#evict(true);\n        }\n      }\n      this.#calculatedSize += sizes[index];\n      if (status) {\n        status.entrySize = size;\n        status.totalCalculatedSize = this.#calculatedSize;\n      }\n    };\n  }\n  #removeItemSize = _i => {};\n  #addItemSize = (_i, _s, _st) => {};\n  #requireSize = (_k, _v, size, sizeCalculation) => {\n    if (size || sizeCalculation) {\n      throw new TypeError('cannot set size without setting maxSize or maxEntrySize on cache');\n    }\n    return 0;\n  };\n  *#indexes({\n    allowStale = this.allowStale\n  } = {}) {\n    if (this.#size) {\n      for (let i = this.#tail; true;) {\n        if (!this.#isValidIndex(i)) {\n          break;\n        }\n        if (allowStale || !this.#isStale(i)) {\n          yield i;\n        }\n        if (i === this.#head) {\n          break;\n        } else {\n          i = this.#prev[i];\n        }\n      }\n    }\n  }\n  *#rindexes({\n    allowStale = this.allowStale\n  } = {}) {\n    if (this.#size) {\n      for (let i = this.#head; true;) {\n        if (!this.#isValidIndex(i)) {\n          break;\n        }\n        if (allowStale || !this.#isStale(i)) {\n          yield i;\n        }\n        if (i === this.#tail) {\n          break;\n        } else {\n          i = this.#next[i];\n        }\n      }\n    }\n  }\n  #isValidIndex(index) {\n    return index !== undefined && this.#keyMap.get(this.#keyList[index]) === index;\n  }\n  /**\n   * Return a generator yielding `[key, value]` pairs,\n   * in order from most recently used to least recently used.\n   */\n  *entries() {\n    for (const i of this.#indexes()) {\n      if (this.#valList[i] !== undefined && this.#keyList[i] !== undefined && !this.#isBackgroundFetch(this.#valList[i])) {\n        yield [this.#keyList[i], this.#valList[i]];\n      }\n    }\n  }\n  /**\n   * Inverse order version of {@link LRUCache.entries}\n   *\n   * Return a generator yielding `[key, value]` pairs,\n   * in order from least recently used to most recently used.\n   */\n  *rentries() {\n    for (const i of this.#rindexes()) {\n      if (this.#valList[i] !== undefined && this.#keyList[i] !== undefined && !this.#isBackgroundFetch(this.#valList[i])) {\n        yield [this.#keyList[i], this.#valList[i]];\n      }\n    }\n  }\n  /**\n   * Return a generator yielding the keys in the cache,\n   * in order from most recently used to least recently used.\n   */\n  *keys() {\n    for (const i of this.#indexes()) {\n      const k = this.#keyList[i];\n      if (k !== undefined && !this.#isBackgroundFetch(this.#valList[i])) {\n        yield k;\n      }\n    }\n  }\n  /**\n   * Inverse order version of {@link LRUCache.keys}\n   *\n   * Return a generator yielding the keys in the cache,\n   * in order from least recently used to most recently used.\n   */\n  *rkeys() {\n    for (const i of this.#rindexes()) {\n      const k = this.#keyList[i];\n      if (k !== undefined && !this.#isBackgroundFetch(this.#valList[i])) {\n        yield k;\n      }\n    }\n  }\n  /**\n   * Return a generator yielding the values in the cache,\n   * in order from most recently used to least recently used.\n   */\n  *values() {\n    for (const i of this.#indexes()) {\n      const v = this.#valList[i];\n      if (v !== undefined && !this.#isBackgroundFetch(this.#valList[i])) {\n        yield this.#valList[i];\n      }\n    }\n  }\n  /**\n   * Inverse order version of {@link LRUCache.values}\n   *\n   * Return a generator yielding the values in the cache,\n   * in order from least recently used to most recently used.\n   */\n  *rvalues() {\n    for (const i of this.#rindexes()) {\n      const v = this.#valList[i];\n      if (v !== undefined && !this.#isBackgroundFetch(this.#valList[i])) {\n        yield this.#valList[i];\n      }\n    }\n  }\n  /**\n   * Iterating over the cache itself yields the same results as\n   * {@link LRUCache.entries}\n   */\n  [Symbol.iterator]() {\n    return this.entries();\n  }\n  /**\n   * A String value that is used in the creation of the default string\n   * description of an object. Called by the built-in method\n   * `Object.prototype.toString`.\n   */\n  [Symbol.toStringTag] = 'LRUCache';\n  /**\n   * Find a value for which the supplied fn method returns a truthy value,\n   * similar to `Array.find()`. fn is called as `fn(value, key, cache)`.\n   */\n  find(fn, getOptions = {}) {\n    for (const i of this.#indexes()) {\n      const v = this.#valList[i];\n      const value = this.#isBackgroundFetch(v) ? v.__staleWhileFetching : v;\n      if (value === undefined) continue;\n      if (fn(value, this.#keyList[i], this)) {\n        return this.get(this.#keyList[i], getOptions);\n      }\n    }\n  }\n  /**\n   * Call the supplied function on each item in the cache, in order from most\n   * recently used to least recently used.\n   *\n   * `fn` is called as `fn(value, key, cache)`.\n   *\n   * If `thisp` is provided, function will be called in the `this`-context of\n   * the provided object, or the cache if no `thisp` object is provided.\n   *\n   * Does not update age or recenty of use, or iterate over stale values.\n   */\n  forEach(fn, thisp = this) {\n    for (const i of this.#indexes()) {\n      const v = this.#valList[i];\n      const value = this.#isBackgroundFetch(v) ? v.__staleWhileFetching : v;\n      if (value === undefined) continue;\n      fn.call(thisp, value, this.#keyList[i], this);\n    }\n  }\n  /**\n   * The same as {@link LRUCache.forEach} but items are iterated over in\n   * reverse order.  (ie, less recently used items are iterated over first.)\n   */\n  rforEach(fn, thisp = this) {\n    for (const i of this.#rindexes()) {\n      const v = this.#valList[i];\n      const value = this.#isBackgroundFetch(v) ? v.__staleWhileFetching : v;\n      if (value === undefined) continue;\n      fn.call(thisp, value, this.#keyList[i], this);\n    }\n  }\n  /**\n   * Delete any stale entries. Returns true if anything was removed,\n   * false otherwise.\n   */\n  purgeStale() {\n    let deleted = false;\n    for (const i of this.#rindexes({\n      allowStale: true\n    })) {\n      if (this.#isStale(i)) {\n        this.#delete(this.#keyList[i], 'expire');\n        deleted = true;\n      }\n    }\n    return deleted;\n  }\n  /**\n   * Get the extended info about a given entry, to get its value, size, and\n   * TTL info simultaneously. Returns `undefined` if the key is not present.\n   *\n   * Unlike {@link LRUCache#dump}, which is designed to be portable and survive\n   * serialization, the `start` value is always the current timestamp, and the\n   * `ttl` is a calculated remaining time to live (negative if expired).\n   *\n   * Always returns stale values, if their info is found in the cache, so be\n   * sure to check for expirations (ie, a negative {@link LRUCache.Entry#ttl})\n   * if relevant.\n   */\n  info(key) {\n    const i = this.#keyMap.get(key);\n    if (i === undefined) return undefined;\n    const v = this.#valList[i];\n    const value = this.#isBackgroundFetch(v) ? v.__staleWhileFetching : v;\n    if (value === undefined) return undefined;\n    const entry = {\n      value\n    };\n    if (this.#ttls && this.#starts) {\n      const ttl = this.#ttls[i];\n      const start = this.#starts[i];\n      if (ttl && start) {\n        const remain = ttl - (perf.now() - start);\n        entry.ttl = remain;\n        entry.start = Date.now();\n      }\n    }\n    if (this.#sizes) {\n      entry.size = this.#sizes[i];\n    }\n    return entry;\n  }\n  /**\n   * Return an array of [key, {@link LRUCache.Entry}] tuples which can be\n   * passed to {@link LRLUCache#load}.\n   *\n   * The `start` fields are calculated relative to a portable `Date.now()`\n   * timestamp, even if `performance.now()` is available.\n   *\n   * Stale entries are always included in the `dump`, even if\n   * {@link LRUCache.OptionsBase.allowStale} is false.\n   *\n   * Note: this returns an actual array, not a generator, so it can be more\n   * easily passed around.\n   */\n  dump() {\n    const arr = [];\n    for (const i of this.#indexes({\n      allowStale: true\n    })) {\n      const key = this.#keyList[i];\n      const v = this.#valList[i];\n      const value = this.#isBackgroundFetch(v) ? v.__staleWhileFetching : v;\n      if (value === undefined || key === undefined) continue;\n      const entry = {\n        value\n      };\n      if (this.#ttls && this.#starts) {\n        entry.ttl = this.#ttls[i];\n        // always dump the start relative to a portable timestamp\n        // it's ok for this to be a bit slow, it's a rare operation.\n        const age = perf.now() - this.#starts[i];\n        entry.start = Math.floor(Date.now() - age);\n      }\n      if (this.#sizes) {\n        entry.size = this.#sizes[i];\n      }\n      arr.unshift([key, entry]);\n    }\n    return arr;\n  }\n  /**\n   * Reset the cache and load in the items in entries in the order listed.\n   *\n   * The shape of the resulting cache may be different if the same options are\n   * not used in both caches.\n   *\n   * The `start` fields are assumed to be calculated relative to a portable\n   * `Date.now()` timestamp, even if `performance.now()` is available.\n   */\n  load(arr) {\n    this.clear();\n    for (const [key, entry] of arr) {\n      if (entry.start) {\n        // entry.start is a portable timestamp, but we may be using\n        // node's performance.now(), so calculate the offset, so that\n        // we get the intended remaining TTL, no matter how long it's\n        // been on ice.\n        //\n        // it's ok for this to be a bit slow, it's a rare operation.\n        const age = Date.now() - entry.start;\n        entry.start = perf.now() - age;\n      }\n      this.set(key, entry.value, entry);\n    }\n  }\n  /**\n   * Add a value to the cache.\n   *\n   * Note: if `undefined` is specified as a value, this is an alias for\n   * {@link LRUCache#delete}\n   *\n   * Fields on the {@link LRUCache.SetOptions} options param will override\n   * their corresponding values in the constructor options for the scope\n   * of this single `set()` operation.\n   *\n   * If `start` is provided, then that will set the effective start\n   * time for the TTL calculation. Note that this must be a previous\n   * value of `performance.now()` if supported, or a previous value of\n   * `Date.now()` if not.\n   *\n   * Options object may also include `size`, which will prevent\n   * calling the `sizeCalculation` function and just use the specified\n   * number if it is a positive integer, and `noDisposeOnSet` which\n   * will prevent calling a `dispose` function in the case of\n   * overwrites.\n   *\n   * If the `size` (or return value of `sizeCalculation`) for a given\n   * entry is greater than `maxEntrySize`, then the item will not be\n   * added to the cache.\n   *\n   * Will update the recency of the entry.\n   *\n   * If the value is `undefined`, then this is an alias for\n   * `cache.delete(key)`. `undefined` is never stored in the cache.\n   */\n  set(k, v, setOptions = {}) {\n    if (v === undefined) {\n      this.delete(k);\n      return this;\n    }\n    const {\n      ttl = this.ttl,\n      start,\n      noDisposeOnSet = this.noDisposeOnSet,\n      sizeCalculation = this.sizeCalculation,\n      status\n    } = setOptions;\n    let {\n      noUpdateTTL = this.noUpdateTTL\n    } = setOptions;\n    const size = this.#requireSize(k, v, setOptions.size || 0, sizeCalculation);\n    // if the item doesn't fit, don't do anything\n    // NB: maxEntrySize set to maxSize by default\n    if (this.maxEntrySize && size > this.maxEntrySize) {\n      if (status) {\n        status.set = 'miss';\n        status.maxEntrySizeExceeded = true;\n      }\n      // have to delete, in case something is there already.\n      this.#delete(k, 'set');\n      return this;\n    }\n    let index = this.#size === 0 ? undefined : this.#keyMap.get(k);\n    if (index === undefined) {\n      // addition\n      index = this.#size === 0 ? this.#tail : this.#free.length !== 0 ? this.#free.pop() : this.#size === this.#max ? this.#evict(false) : this.#size;\n      this.#keyList[index] = k;\n      this.#valList[index] = v;\n      this.#keyMap.set(k, index);\n      this.#next[this.#tail] = index;\n      this.#prev[index] = this.#tail;\n      this.#tail = index;\n      this.#size++;\n      this.#addItemSize(index, size, status);\n      if (status) status.set = 'add';\n      noUpdateTTL = false;\n    } else {\n      // update\n      this.#moveToTail(index);\n      const oldVal = this.#valList[index];\n      if (v !== oldVal) {\n        if (this.#hasFetchMethod && this.#isBackgroundFetch(oldVal)) {\n          oldVal.__abortController.abort(new Error('replaced'));\n          const {\n            __staleWhileFetching: s\n          } = oldVal;\n          if (s !== undefined && !noDisposeOnSet) {\n            if (this.#hasDispose) {\n              this.#dispose?.(s, k, 'set');\n            }\n            if (this.#hasDisposeAfter) {\n              this.#disposed?.push([s, k, 'set']);\n            }\n          }\n        } else if (!noDisposeOnSet) {\n          if (this.#hasDispose) {\n            this.#dispose?.(oldVal, k, 'set');\n          }\n          if (this.#hasDisposeAfter) {\n            this.#disposed?.push([oldVal, k, 'set']);\n          }\n        }\n        this.#removeItemSize(index);\n        this.#addItemSize(index, size, status);\n        this.#valList[index] = v;\n        if (status) {\n          status.set = 'replace';\n          const oldValue = oldVal && this.#isBackgroundFetch(oldVal) ? oldVal.__staleWhileFetching : oldVal;\n          if (oldValue !== undefined) status.oldValue = oldValue;\n        }\n      } else if (status) {\n        status.set = 'update';\n      }\n    }\n    if (ttl !== 0 && !this.#ttls) {\n      this.#initializeTTLTracking();\n    }\n    if (this.#ttls) {\n      if (!noUpdateTTL) {\n        this.#setItemTTL(index, ttl, start);\n      }\n      if (status) this.#statusTTL(status, index);\n    }\n    if (!noDisposeOnSet && this.#hasDisposeAfter && this.#disposed) {\n      const dt = this.#disposed;\n      let task;\n      while (task = dt?.shift()) {\n        this.#disposeAfter?.(...task);\n      }\n    }\n    return this;\n  }\n  /**\n   * Evict the least recently used item, returning its value or\n   * `undefined` if cache is empty.\n   */\n  pop() {\n    try {\n      while (this.#size) {\n        const val = this.#valList[this.#head];\n        this.#evict(true);\n        if (this.#isBackgroundFetch(val)) {\n          if (val.__staleWhileFetching) {\n            return val.__staleWhileFetching;\n          }\n        } else if (val !== undefined) {\n          return val;\n        }\n      }\n    } finally {\n      if (this.#hasDisposeAfter && this.#disposed) {\n        const dt = this.#disposed;\n        let task;\n        while (task = dt?.shift()) {\n          this.#disposeAfter?.(...task);\n        }\n      }\n    }\n  }\n  #evict(free) {\n    const head = this.#head;\n    const k = this.#keyList[head];\n    const v = this.#valList[head];\n    if (this.#hasFetchMethod && this.#isBackgroundFetch(v)) {\n      v.__abortController.abort(new Error('evicted'));\n    } else if (this.#hasDispose || this.#hasDisposeAfter) {\n      if (this.#hasDispose) {\n        this.#dispose?.(v, k, 'evict');\n      }\n      if (this.#hasDisposeAfter) {\n        this.#disposed?.push([v, k, 'evict']);\n      }\n    }\n    this.#removeItemSize(head);\n    // if we aren't about to use the index, then null these out\n    if (free) {\n      this.#keyList[head] = undefined;\n      this.#valList[head] = undefined;\n      this.#free.push(head);\n    }\n    if (this.#size === 1) {\n      this.#head = this.#tail = 0;\n      this.#free.length = 0;\n    } else {\n      this.#head = this.#next[head];\n    }\n    this.#keyMap.delete(k);\n    this.#size--;\n    return head;\n  }\n  /**\n   * Check if a key is in the cache, without updating the recency of use.\n   * Will return false if the item is stale, even though it is technically\n   * in the cache.\n   *\n   * Check if a key is in the cache, without updating the recency of\n   * use. Age is updated if {@link LRUCache.OptionsBase.updateAgeOnHas} is set\n   * to `true` in either the options or the constructor.\n   *\n   * Will return `false` if the item is stale, even though it is technically in\n   * the cache. The difference can be determined (if it matters) by using a\n   * `status` argument, and inspecting the `has` field.\n   *\n   * Will not update item age unless\n   * {@link LRUCache.OptionsBase.updateAgeOnHas} is set.\n   */\n  has(k, hasOptions = {}) {\n    const {\n      updateAgeOnHas = this.updateAgeOnHas,\n      status\n    } = hasOptions;\n    const index = this.#keyMap.get(k);\n    if (index !== undefined) {\n      const v = this.#valList[index];\n      if (this.#isBackgroundFetch(v) && v.__staleWhileFetching === undefined) {\n        return false;\n      }\n      if (!this.#isStale(index)) {\n        if (updateAgeOnHas) {\n          this.#updateItemAge(index);\n        }\n        if (status) {\n          status.has = 'hit';\n          this.#statusTTL(status, index);\n        }\n        return true;\n      } else if (status) {\n        status.has = 'stale';\n        this.#statusTTL(status, index);\n      }\n    } else if (status) {\n      status.has = 'miss';\n    }\n    return false;\n  }\n  /**\n   * Like {@link LRUCache#get} but doesn't update recency or delete stale\n   * items.\n   *\n   * Returns `undefined` if the item is stale, unless\n   * {@link LRUCache.OptionsBase.allowStale} is set.\n   */\n  peek(k, peekOptions = {}) {\n    const {\n      allowStale = this.allowStale\n    } = peekOptions;\n    const index = this.#keyMap.get(k);\n    if (index === undefined || !allowStale && this.#isStale(index)) {\n      return;\n    }\n    const v = this.#valList[index];\n    // either stale and allowed, or forcing a refresh of non-stale value\n    return this.#isBackgroundFetch(v) ? v.__staleWhileFetching : v;\n  }\n  #backgroundFetch(k, index, options, context) {\n    const v = index === undefined ? undefined : this.#valList[index];\n    if (this.#isBackgroundFetch(v)) {\n      return v;\n    }\n    const ac = new AC();\n    const {\n      signal\n    } = options;\n    // when/if our AC signals, then stop listening to theirs.\n    signal?.addEventListener('abort', () => ac.abort(signal.reason), {\n      signal: ac.signal\n    });\n    const fetchOpts = {\n      signal: ac.signal,\n      options,\n      context\n    };\n    const cb = (v, updateCache = false) => {\n      const {\n        aborted\n      } = ac.signal;\n      const ignoreAbort = options.ignoreFetchAbort && v !== undefined;\n      if (options.status) {\n        if (aborted && !updateCache) {\n          options.status.fetchAborted = true;\n          options.status.fetchError = ac.signal.reason;\n          if (ignoreAbort) options.status.fetchAbortIgnored = true;\n        } else {\n          options.status.fetchResolved = true;\n        }\n      }\n      if (aborted && !ignoreAbort && !updateCache) {\n        return fetchFail(ac.signal.reason);\n      }\n      // either we didn't abort, and are still here, or we did, and ignored\n      const bf = p;\n      if (this.#valList[index] === p) {\n        if (v === undefined) {\n          if (bf.__staleWhileFetching) {\n            this.#valList[index] = bf.__staleWhileFetching;\n          } else {\n            this.#delete(k, 'fetch');\n          }\n        } else {\n          if (options.status) options.status.fetchUpdated = true;\n          this.set(k, v, fetchOpts.options);\n        }\n      }\n      return v;\n    };\n    const eb = er => {\n      if (options.status) {\n        options.status.fetchRejected = true;\n        options.status.fetchError = er;\n      }\n      return fetchFail(er);\n    };\n    const fetchFail = er => {\n      const {\n        aborted\n      } = ac.signal;\n      const allowStaleAborted = aborted && options.allowStaleOnFetchAbort;\n      const allowStale = allowStaleAborted || options.allowStaleOnFetchRejection;\n      const noDelete = allowStale || options.noDeleteOnFetchRejection;\n      const bf = p;\n      if (this.#valList[index] === p) {\n        // if we allow stale on fetch rejections, then we need to ensure that\n        // the stale value is not removed from the cache when the fetch fails.\n        const del = !noDelete || bf.__staleWhileFetching === undefined;\n        if (del) {\n          this.#delete(k, 'fetch');\n        } else if (!allowStaleAborted) {\n          // still replace the *promise* with the stale value,\n          // since we are done with the promise at this point.\n          // leave it untouched if we're still waiting for an\n          // aborted background fetch that hasn't yet returned.\n          this.#valList[index] = bf.__staleWhileFetching;\n        }\n      }\n      if (allowStale) {\n        if (options.status && bf.__staleWhileFetching !== undefined) {\n          options.status.returnedStale = true;\n        }\n        return bf.__staleWhileFetching;\n      } else if (bf.__returned === bf) {\n        throw er;\n      }\n    };\n    const pcall = (res, rej) => {\n      const fmp = this.#fetchMethod?.(k, v, fetchOpts);\n      if (fmp && fmp instanceof Promise) {\n        fmp.then(v => res(v === undefined ? undefined : v), rej);\n      }\n      // ignored, we go until we finish, regardless.\n      // defer check until we are actually aborting,\n      // so fetchMethod can override.\n      ac.signal.addEventListener('abort', () => {\n        if (!options.ignoreFetchAbort || options.allowStaleOnFetchAbort) {\n          res(undefined);\n          // when it eventually resolves, update the cache.\n          if (options.allowStaleOnFetchAbort) {\n            res = v => cb(v, true);\n          }\n        }\n      });\n    };\n    if (options.status) options.status.fetchDispatched = true;\n    const p = new Promise(pcall).then(cb, eb);\n    const bf = Object.assign(p, {\n      __abortController: ac,\n      __staleWhileFetching: v,\n      __returned: undefined\n    });\n    if (index === undefined) {\n      // internal, don't expose status.\n      this.set(k, bf, {\n        ...fetchOpts.options,\n        status: undefined\n      });\n      index = this.#keyMap.get(k);\n    } else {\n      this.#valList[index] = bf;\n    }\n    return bf;\n  }\n  #isBackgroundFetch(p) {\n    if (!this.#hasFetchMethod) return false;\n    const b = p;\n    return !!b && b instanceof Promise && b.hasOwnProperty('__staleWhileFetching') && b.__abortController instanceof AC;\n  }\n  async fetch(k, fetchOptions = {}) {\n    const {\n      // get options\n      allowStale = this.allowStale,\n      updateAgeOnGet = this.updateAgeOnGet,\n      noDeleteOnStaleGet = this.noDeleteOnStaleGet,\n      // set options\n      ttl = this.ttl,\n      noDisposeOnSet = this.noDisposeOnSet,\n      size = 0,\n      sizeCalculation = this.sizeCalculation,\n      noUpdateTTL = this.noUpdateTTL,\n      // fetch exclusive options\n      noDeleteOnFetchRejection = this.noDeleteOnFetchRejection,\n      allowStaleOnFetchRejection = this.allowStaleOnFetchRejection,\n      ignoreFetchAbort = this.ignoreFetchAbort,\n      allowStaleOnFetchAbort = this.allowStaleOnFetchAbort,\n      context,\n      forceRefresh = false,\n      status,\n      signal\n    } = fetchOptions;\n    if (!this.#hasFetchMethod) {\n      if (status) status.fetch = 'get';\n      return this.get(k, {\n        allowStale,\n        updateAgeOnGet,\n        noDeleteOnStaleGet,\n        status\n      });\n    }\n    const options = {\n      allowStale,\n      updateAgeOnGet,\n      noDeleteOnStaleGet,\n      ttl,\n      noDisposeOnSet,\n      size,\n      sizeCalculation,\n      noUpdateTTL,\n      noDeleteOnFetchRejection,\n      allowStaleOnFetchRejection,\n      allowStaleOnFetchAbort,\n      ignoreFetchAbort,\n      status,\n      signal\n    };\n    let index = this.#keyMap.get(k);\n    if (index === undefined) {\n      if (status) status.fetch = 'miss';\n      const p = this.#backgroundFetch(k, index, options, context);\n      return p.__returned = p;\n    } else {\n      // in cache, maybe already fetching\n      const v = this.#valList[index];\n      if (this.#isBackgroundFetch(v)) {\n        const stale = allowStale && v.__staleWhileFetching !== undefined;\n        if (status) {\n          status.fetch = 'inflight';\n          if (stale) status.returnedStale = true;\n        }\n        return stale ? v.__staleWhileFetching : v.__returned = v;\n      }\n      // if we force a refresh, that means do NOT serve the cached value,\n      // unless we are already in the process of refreshing the cache.\n      const isStale = this.#isStale(index);\n      if (!forceRefresh && !isStale) {\n        if (status) status.fetch = 'hit';\n        this.#moveToTail(index);\n        if (updateAgeOnGet) {\n          this.#updateItemAge(index);\n        }\n        if (status) this.#statusTTL(status, index);\n        return v;\n      }\n      // ok, it is stale or a forced refresh, and not already fetching.\n      // refresh the cache.\n      const p = this.#backgroundFetch(k, index, options, context);\n      const hasStale = p.__staleWhileFetching !== undefined;\n      const staleVal = hasStale && allowStale;\n      if (status) {\n        status.fetch = isStale ? 'stale' : 'refresh';\n        if (staleVal && isStale) status.returnedStale = true;\n      }\n      return staleVal ? p.__staleWhileFetching : p.__returned = p;\n    }\n  }\n  async forceFetch(k, fetchOptions = {}) {\n    const v = await this.fetch(k, fetchOptions);\n    if (v === undefined) throw new Error('fetch() returned undefined');\n    return v;\n  }\n  memo(k, memoOptions = {}) {\n    const memoMethod = this.#memoMethod;\n    if (!memoMethod) {\n      throw new Error('no memoMethod provided to constructor');\n    }\n    const {\n      context,\n      forceRefresh,\n      ...options\n    } = memoOptions;\n    const v = this.get(k, options);\n    if (!forceRefresh && v !== undefined) return v;\n    const vv = memoMethod(k, v, {\n      options,\n      context\n    });\n    this.set(k, vv, options);\n    return vv;\n  }\n  /**\n   * Return a value from the cache. Will update the recency of the cache\n   * entry found.\n   *\n   * If the key is not found, get() will return `undefined`.\n   */\n  get(k, getOptions = {}) {\n    const {\n      allowStale = this.allowStale,\n      updateAgeOnGet = this.updateAgeOnGet,\n      noDeleteOnStaleGet = this.noDeleteOnStaleGet,\n      status\n    } = getOptions;\n    const index = this.#keyMap.get(k);\n    if (index !== undefined) {\n      const value = this.#valList[index];\n      const fetching = this.#isBackgroundFetch(value);\n      if (status) this.#statusTTL(status, index);\n      if (this.#isStale(index)) {\n        if (status) status.get = 'stale';\n        // delete only if not an in-flight background fetch\n        if (!fetching) {\n          if (!noDeleteOnStaleGet) {\n            this.#delete(k, 'expire');\n          }\n          if (status && allowStale) status.returnedStale = true;\n          return allowStale ? value : undefined;\n        } else {\n          if (status && allowStale && value.__staleWhileFetching !== undefined) {\n            status.returnedStale = true;\n          }\n          return allowStale ? value.__staleWhileFetching : undefined;\n        }\n      } else {\n        if (status) status.get = 'hit';\n        // if we're currently fetching it, we don't actually have it yet\n        // it's not stale, which means this isn't a staleWhileRefetching.\n        // If it's not stale, and fetching, AND has a __staleWhileFetching\n        // value, then that means the user fetched with {forceRefresh:true},\n        // so it's safe to return that value.\n        if (fetching) {\n          return value.__staleWhileFetching;\n        }\n        this.#moveToTail(index);\n        if (updateAgeOnGet) {\n          this.#updateItemAge(index);\n        }\n        return value;\n      }\n    } else if (status) {\n      status.get = 'miss';\n    }\n  }\n  #connect(p, n) {\n    this.#prev[n] = p;\n    this.#next[p] = n;\n  }\n  #moveToTail(index) {\n    // if tail already, nothing to do\n    // if head, move head to next[index]\n    // else\n    //   move next[prev[index]] to next[index] (head has no prev)\n    //   move prev[next[index]] to prev[index]\n    // prev[index] = tail\n    // next[tail] = index\n    // tail = index\n    if (index !== this.#tail) {\n      if (index === this.#head) {\n        this.#head = this.#next[index];\n      } else {\n        this.#connect(this.#prev[index], this.#next[index]);\n      }\n      this.#connect(this.#tail, index);\n      this.#tail = index;\n    }\n  }\n  /**\n   * Deletes a key out of the cache.\n   *\n   * Returns true if the key was deleted, false otherwise.\n   */\n  delete(k) {\n    return this.#delete(k, 'delete');\n  }\n  #delete(k, reason) {\n    let deleted = false;\n    if (this.#size !== 0) {\n      const index = this.#keyMap.get(k);\n      if (index !== undefined) {\n        deleted = true;\n        if (this.#size === 1) {\n          this.#clear(reason);\n        } else {\n          this.#removeItemSize(index);\n          const v = this.#valList[index];\n          if (this.#isBackgroundFetch(v)) {\n            v.__abortController.abort(new Error('deleted'));\n          } else if (this.#hasDispose || this.#hasDisposeAfter) {\n            if (this.#hasDispose) {\n              this.#dispose?.(v, k, reason);\n            }\n            if (this.#hasDisposeAfter) {\n              this.#disposed?.push([v, k, reason]);\n            }\n          }\n          this.#keyMap.delete(k);\n          this.#keyList[index] = undefined;\n          this.#valList[index] = undefined;\n          if (index === this.#tail) {\n            this.#tail = this.#prev[index];\n          } else if (index === this.#head) {\n            this.#head = this.#next[index];\n          } else {\n            const pi = this.#prev[index];\n            this.#next[pi] = this.#next[index];\n            const ni = this.#next[index];\n            this.#prev[ni] = this.#prev[index];\n          }\n          this.#size--;\n          this.#free.push(index);\n        }\n      }\n    }\n    if (this.#hasDisposeAfter && this.#disposed?.length) {\n      const dt = this.#disposed;\n      let task;\n      while (task = dt?.shift()) {\n        this.#disposeAfter?.(...task);\n      }\n    }\n    return deleted;\n  }\n  /**\n   * Clear the cache entirely, throwing away all values.\n   */\n  clear() {\n    return this.#clear('delete');\n  }\n  #clear(reason) {\n    for (const index of this.#rindexes({\n      allowStale: true\n    })) {\n      const v = this.#valList[index];\n      if (this.#isBackgroundFetch(v)) {\n        v.__abortController.abort(new Error('deleted'));\n      } else {\n        const k = this.#keyList[index];\n        if (this.#hasDispose) {\n          this.#dispose?.(v, k, reason);\n        }\n        if (this.#hasDisposeAfter) {\n          this.#disposed?.push([v, k, reason]);\n        }\n      }\n    }\n    this.#keyMap.clear();\n    this.#valList.fill(undefined);\n    this.#keyList.fill(undefined);\n    if (this.#ttls && this.#starts) {\n      this.#ttls.fill(0);\n      this.#starts.fill(0);\n    }\n    if (this.#sizes) {\n      this.#sizes.fill(0);\n    }\n    this.#head = 0;\n    this.#tail = 0;\n    this.#free.length = 0;\n    this.#calculatedSize = 0;\n    this.#size = 0;\n    if (this.#hasDisposeAfter && this.#disposed) {\n      const dt = this.#disposed;\n      let task;\n      while (task = dt?.shift()) {\n        this.#disposeAfter?.(...task);\n      }\n    }\n  }\n}\n//# sourceMappingURL=index.js.map","map":null,"metadata":{},"sourceType":"module","externalDependencies":[]}